---
theme: default
class: text-center
highlighter: shiki
lineNumbers: false
drawings:
  persist: false
transition: slide-left
title: 'Bloc 1: Fondations & Histoire'
mdc: true
---

# Projet IA & Big Data

ESIGELEC Poitiers - Dominante IA & Big Data

---
layout: two-cols-header
---

# Qui suis-je?

- ğŸ§‘ğŸ¾ **Nom**: Brice FOTZO
- ğŸŒ **Origine**: Bafoussam, Cameroun
- ğŸ“ **Formation**: IngÃ©nieur GÃ©nÃ©raliste(BDTN) - ESIGELEC Rouen

> Parcours Data & AI
- Analytics Engineer - **Apprenti** @ Renault
- Data Engineer - CDI @ Starclay
- Tech Lead Data - CDI @ HephIA
- Tech Lead Data - CDI @ Servier
- Intervenant - CDD @ ESIGELEC Rouen/Poitiers

> Engagements Communautaires
- Data Engineer - **Volunteer** @ Validalab(DFG)
- MUG Leader Paris - **Volunteer** @ MongoDB
- CrÃ©ateur de contenu @ LinkedIn, Medium
- CrÃ©ateur de Serial Techos

---
layout: default
---

# Et vous?
## Faisons connaissance

<div class="text-xl mt-8">

- Votre nom et parcours?
- Vos expÃ©riences en IA/Data?
- Vos attentes pour ce cours?
- Projets personnels ou professionnels?

</div>

---
layout: two-cols-header
---

# Objectifs d'apprentissage

::left::

**CompÃ©tences techniques**
- Comprendre l'exÃ©cution des modÃ¨les LLM
- Construire et gÃ©rer un stockage vectoriel
- MaÃ®triser le RAG (Retrieval Augmented Generation)
- Optimiser l'infÃ©rence (Flash Attention, KV cache)
- DÃ©ployer les LLM (local, serveur, edge)
- SÃ©curiser les applications LLM

::right::

**Technologies et outils**
- API LLM et modÃ¨les open-source
- Bases de donnÃ©es vectorielles
- ModÃ¨les d'embedding
- Techniques de prompt engineering
- Frameworks de dÃ©ploiement
- Mesures de sÃ©curitÃ© et surveillance

---
layout: two-cols-header
---

# Structure du cours

::left::

**Volume horaire**
- 6h de cours magistraux
- 44h de travaux pratiques
- **Total**: 50h

**Ã‰valuation**
- 1 ContrÃ´le Continu (50%)
- 1 ContrÃ´le Final (50%)

::right::

**Ressources**
- Support de cours - Slides
- Exercices pratiques - GitHub
- Environnement de dÃ©veloppement
- Documentation en ligne
- Exemples de code

---
layout: center
---

# Plan (1h)

<div class="text-2xl mt-12 space-y-6">

**1.1** Un peu d'histoire

**1.2** Les ModÃ¨les 

**1.3** Ã‰valuation 

</div>

---
layout: section
---

# 1.1
# Un peu d'histoire

---
layout: center
---

# Ã‰volution de l'IA

<div class="text-3xl mt-16 space-y-8">

**1950s-1990s** â†’ SystÃ¨mes experts

**2000s** â†’ Machine Learning statistique

**2010s** â†’ Deep Learning

**2017+** â†’ `Era Transformer` âš¡ 

**2020s** â†’ Generative AI

</div>

<style>
.space-y-8 > * + * {
  margin-top: 2rem !important;
}
</style>

---
layout: center
---

# ğŸ”¥ 2017 : Moment ClÃ©

<div class="mt-20">
<h2 class="text-5xl font-bold mb-8">"Attention Is All You Need"</h2>

<div class="text-2xl space-y-4">

**8 auteurs** Â· Google Brain/Research

**173,000+** citations (2025)

</div>
</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
ğŸ“ Notez : Architecture Transformer
</div>


---
layout: center
---

# 3 Enablers Techniques

<div class="grid grid-cols-3 gap-16 mt-16 text-center">

<div>
<div class="text-6xl mb-4">ğŸ“Š</div>
<div class="text-2xl font-bold">Big Data</div>
<div class="text-lg mt-4 text-gray-600">PÃ©taoctets<br/>Common Crawl</div>
</div>

<div>
<div class="text-6xl mb-4">âš¡</div>
<div class="text-2xl font-bold">Calcul<br/>DistribuÃ©</div>
<div class="text-lg mt-4 text-gray-600">Spark â†’ Ray<br/>Parallelism</div>
</div>

<div>
<div class="text-6xl mb-4">ğŸ–¥ï¸</div>
<div class="text-2xl font-bold">Compute<br/>Power</div>
<div class="text-lg mt-4 text-gray-600">GPU â†’ TPU<br/>H100 clusters</div>
</div>

</div>

---
layout: center
---

# Ã‰volution des Cas d'Usage

<div class="flex justify-center items-center gap-12 mt-20 text-3xl">

**PrÃ©diction**

<div class="text-5xl">â†’</div>

**GÃ©nÃ©ration**

<div class="text-5xl">â†’</div>

**Raisonnement**

</div>

<div class="flex justify-center gap-12 mt-12 text-lg text-gray-600">

<div>Classification<br/>Scoring</div>

<div>GPT, DALL-E<br/>Creative AI</div>

<div>o3, DeepSeek R1<br/>Agents</div>

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
ğŸ“ Notez : les 3 eras
</div>

---
layout: section
---

# 1.2
# Les ModÃ¨les

---
layout: two-cols
---

# Innovation Transformer

<div class="text-xl mt-8 space-y-6">

âœ¨ **Self-Attention**

âš¡ **ParallÃ©lisation** massive

ğŸš« **Supprime** RNN/LSTM

ğŸ“ˆ Performance supÃ©rieure

</div>

::right::

<div class="mt-8 ml-8">

### RÃ©sultat

<div class="text-6xl font-bold text-blue-600 mt-12">
28.4
</div>

<div class="text-xl mt-4">
BLEU score
<div class="text-sm  text-gray-500">
Bilingual Evaluation Understudy
</div>

Englishâ†’German
</div>

<div class="text-sm m text-green-500">
+2 points vs Ã©tat de l'art
</div>

</div>

---
layout: two-cols-header
---

# L'Attention : Quelle Innovation ?

::left::

### âŒ Avant (RNN/LSTM)

<div class="text-lg mt-6 space-y-4">

**Traitement sÃ©quentiel**
```
"Le chat" â†’ "mange" â†’ "la" â†’ "souris"
   â†“         â†“        â†“        â†“
  h1  â†’    h2  â†’   h3  â†’    h4
```

**ProblÃ¨mes:**
- Traite mot par mot ğŸŒ
- Oublie le dÃ©but de la phrase
- Pas de parallÃ©lisation
- "Le chat" â†’ loin de "souris"

</div>


::right::

### âœ… Avec Attention

<div class="text-lg mt-6 space-y-4">

**Traitement parallÃ¨le**
```
"Le chat mange la souris"
  â†•ï¸    â†•ï¸    â†•ï¸   â†•ï¸    â†•ï¸
Tous les mots se "voient"
```

**Avantages:**
- AccÃ¨s direct Ã  tous les mots âš¡
- Comprend les relations longue distance
- ParallÃ©lisable sur GPU
- "Le chat" â†”ï¸ "souris" : lien direct

</div>

---
layout: center
---

# Exemple Concret : Traduction

<div class="mt-12 space-y-12">

<div class="text-2xl">
ğŸ‡«ğŸ‡· "Le directeur de la banque qui est grand"
</div>

<div class="text-lg text-gray-600">
Qui est grand ? Le directeur ou la banque ?
</div>

<div class="mt-8 p-6 bg-blue-50 rounded-lg text-gray-600">
<div class="font-bold mb-4">ğŸ’¡ L'Attention rÃ©sout ce problÃ¨me :</div>
<div class="text-lg">
Elle calcule un <span class="font-bold text-blue-600">"score d'importance"</span> entre chaque paire de mots
<br/><br/>
â†’ "grand" regarde tous les mots et dÃ©cide : <span class="font-bold">"directeur"</span> (score Ã©levÃ©) vs "banque" (score faible)
</div>
</div>

</div>

---
layout: center
---

# 2 Familles Principales

<div class="grid grid-cols-2 gap-20 mt-20 text-center">

<div>
<div class="text-8xl mb-6">ğŸ¯</div>
<h2 class="text-4xl font-bold mb-4">SLM</h2>
<div class="text-xl text-gray-600">Small Language<br/>Models</div>
<div class="mt-6 text-lg">1B - 14B params</div>
</div>

<div>
<div class="text-8xl mb-6">ğŸŒŸ</div>
<h2 class="text-4xl font-bold mb-4">LLM</h2>
<div class="text-xl text-gray-600">Large Language<br/>Models</div>
<div class="mt-6 text-lg">70B - 671B params</div>
</div>

</div>

---
layout: two-cols
---

# Small Language Models

<div class="text-xl mt-8 space-y-6">

### Principe

**Distillation**
Teacher â†’ Student

### Exemple : Phi-4

ğŸ“Š **14B** paramÃ¨tres

ğŸ“ **SpÃ©cialitÃ©** : Math

ğŸ† Surpasse modÃ¨les 5x plus gros

</div>

::right::

<div class="mt-8 ml-8">

### Avantages

<div class="text-2xl space-y-6 mt-12">

ğŸ“± **Edge / Local**

ğŸ’° **CoÃ»ts rÃ©duits**

âš¡ **Latence faible**

ğŸ”’ **Privacy**

</div>

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
ğŸ“ Notez : Distillation teacher-student
</div>

<!--
La distillation (ou knowledge distillation) vient du Machine Learning classique.
Câ€™est une technique pour transfÃ©rer le savoir dâ€™un grand modÃ¨le (â€œteacherâ€) vers un plus petit (â€œstudentâ€).

Teacher = un modÃ¨le trÃ¨s puissant, lent, coÃ»teux

Student = un modÃ¨le plus petit, plus rapide, quâ€™on veut rendre presque aussi bon

ğŸ’¡ But : rÃ©duire la taille et le coÃ»t, sans perdre trop de performance.
-->

---
layout: center
---

# Phi-4 Performance

<div class="mt-16">

| Benchmark | Phi-4 (14B) | Llama 3.3 (70B) |
|-----------|-------------|-----------------|
| GPQA | **75.5%** | - |
| MATH-500 | **94%+** | ~85% |
| MMLU | **88.2%** | 86.3% |

</div>

<div class="text-center mt-12 text-2xl">
ğŸ’¡ 5x plus petit, performances comparables
</div>

<!--
ğŸ§  1. MMLU â€” Massive Multitask Language Understanding

ğŸ‘‰ But : tester la culture gÃ©nÃ©rale et la comprÃ©hension dâ€™un modÃ¨le.
57 domaines : math, mÃ©decine, droit, informatique, etc.
Niveau : collÃ¨ge â†’ doctorat

2. GPQA â€” Graduate-Level Google-Proof Q&A

ğŸ‘‰ But : Ã©valuer la raisonnement scientifique de haut niveau.
Questions validÃ©es par des chercheurs (PhD)
Domaines : physique, biologie, chimie

ğŸ”¢ 3. MATH-500
But : Ã©valuer la rÃ©solution dâ€™exercices mathÃ©matiques (niveau lycÃ©e Ã  universitÃ©).
500 problÃ¨mes, souvent issus dâ€™olympiades ou concours.
VÃ©rifie le raisonnement pas Ã  pas.
-->

---
layout: center
---

# LLM : 2 CatÃ©gories

<div class="grid grid-cols-2 gap-16 mt-20 text-center">

<div class="p-8 bg-gradient-to-br from-purple-500 to-pink-500 text-white rounded-xl">
<div class="text-3xl font-bold mb-4">PropriÃ©taires</div>
<div class="text-xl space-y-3 mt-8">
<div>OpenAI GPT-4o, o3</div>
<div>Anthropic Claude 4</div>
<div>Google Gemini 2.5</div>
</div>
</div>

<div class="p-8 bg-gradient-to-br from-blue-500 to-cyan-500 text-white rounded-xl">
<div class="text-3xl font-bold mb-4">Open-Weight</div>
<div class="text-xl space-y-3 mt-8">
<div>Meta Llama 3.3</div>
<div>Mistral Large 2</div>
<div>Qwen 2.5</div>
</div>
</div>

</div>

---
layout: center
---

# OpenAI : 2 Lignes

<div class="grid grid-cols-2 gap-12 mt-16">

<div class="text-center">
<div class="text-5xl mb-6">ğŸ¨</div>
<h2 class="text-3xl font-bold mb-6">GPT-4o</h2>
<div class="text-xl space-y-4">
<div>Multimodal</div>
<div>Vision + Audio</div>
<div>232ms latency</div>
</div>
</div>

<div class="text-center">
<div class="text-5xl mb-6">ğŸ§ </div>
<h2 class="text-3xl font-bold mb-6">o3 / o4-mini</h2>
<div class="text-xl space-y-4">
<div>Reasoning</div>
<div>Chain-of-Thought</div>
<div>Math + Code</div>
</div>
</div>

</div>

---
layout: center
---

# Claude 4 : 3 Tailles

<div class="space-y-10 mt-12 text-2xl">

**Opus 4** â†’ Meilleur coding au monde
<div class="text-lg text-gray-600">72.5% SWE-bench Â· 7h autonomie</div>

**Sonnet 4.5** â†’ Meilleur Ã©quilibre
<div class="text-lg text-gray-600">30h+ autonomie Â· $3/$15</div>

**Haiku 4.5** â†’ Ultra rapide
<div class="text-lg text-gray-600">2x+ rapide Â· 1/3 du coÃ»t Â· $1/$5</div>

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
ğŸ“ Notez : Opus/Sonnet/Haiku
</div>

---
layout: center
---

# ğŸ§  ModÃ¨les de Raisonnement

<div class="text-3xl mt-16 space-y-10">

**OpenAI o3**
<div class="text-xl text-gray-600">87.7% GPQA Diamond Â· PhD-level</div>

**DeepSeek R1**
<div class="text-xl text-gray-600">Open-source Â· 15-50% du coÃ»t Â· 671B params</div>

**Claude 4** (hybrid)
<div class="text-xl text-gray-600">Extended thinking + Tools</div>

</div>

---
layout: center
---

# DeepSeek R1 : Innovation

<div class="mt-16 space-y-12 text-2xl">

ğŸ”¬ **Pure RL** (sans SFT initial)

ğŸ¯ **671B params** â†’ 37B actifs (MoE)

ğŸ’° **Cost:** 15-50% d'OpenAI

ğŸ“œ **Open-source** (MIT License)

</div>

<div class="text-center mt-16 text-3xl font-bold">
79.8% AIME 2024
</div>


<div class="absolute left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
ğŸ“ Notez : RL pur + MoE
</div>

<!--
ğŸ”¬ 1. â€œPure RL (sans SFT initial)â€

ğŸ‘‰ RL = Reinforcement Learning (apprentissage par renforcement).
ğŸ‘‰ SFT = Supervised Fine-Tuning (apprentissage supervisÃ©).

La plupart des LLM (ex : GPT-4, Claude 4) suivent ce pipeline :

PrÃ©-entraÃ®nement sur du texte brut.

SFT â†’ on les affine avec des exemples humains (â€œbonne rÃ©ponse / mauvaise rÃ©ponseâ€).

RLHF (Reinforcement Learning from Human Feedback) â†’ on les aligne avec ce que les humains prÃ©fÃ¨rent.

ğŸ’¡ DeepSeek R1 saute lâ€™Ã©tape SFT :
il apprend directement par renforcement, en explorant et en recevant une rÃ©compense selon la qualitÃ© de sa rÃ©ponse.
â†’ Câ€™est ce quâ€™on appelle un modÃ¨le â€œpure RLâ€.

Avantage : il dÃ©couvre des stratÃ©gies originales.
Risque : apprentissage plus instable, car pas guidÃ© au dÃ©part.

ğŸ¯ 2. â€œ671B params â†’ 37B actifs (MoE)â€

671B paramÃ¨tres = taille totale du modÃ¨le.

Mais seulement 37 milliards sont activÃ©s Ã  chaque requÃªte.
â†’ Câ€™est ce quâ€™on appelle un Mixture of Experts (MoE).

ğŸ’¡ MoE = au lieu dâ€™utiliser tout le cerveau du modÃ¨le Ã  chaque fois,
il active seulement les â€œexpertsâ€ nÃ©cessaires pour une tÃ¢che donnÃ©e.
Câ€™est comme un grand groupe dâ€™experts, oÃ¹ seuls ceux concernÃ©s parlent.

ğŸ‘‰ RÃ©sultat : Ã©norme modÃ¨le, mais coÃ»t dâ€™infÃ©rence rÃ©duit.
-->

---
layout: center
---

# Embeddings : ReprÃ©sentation Vectorielle

<div class="mt-16 text-center">

<div class="text-5xl mb-8">ğŸ“Š</div>

<div class="text-3xl mb-12">
Texte â†’ Vecteur dense
</div>

<div class="text-2xl space-y-6">

**256 Ã  3072** dimensions

**Cosine similarity** pour comparer

**Usage:** RAG, Search, Clustering

</div>

</div>

---
layout: center
---

# Top Embedding Models (2025)

<div class="mt-12  text-2xl">

ğŸ† **Voyage AI voyage-3-large**
<div class="text-lg text-gray-600">#1 MTEB Â· 32K context Â· +9.74% vs OpenAI</div>

**OpenAI text-embedding-3**
<div class="text-lg text-gray-600">3072 dims Â· 8K context Â· Standard industrie</div>

**Cohere embed-v3**
<div class="text-lg text-gray-600">Multilingual Â· 100+ langues</div>

**Mistral-embed**
<div class="text-lg text-gray-600">77.8% accuracy Â· Cost-effective</div>

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
ğŸ“ Notez : MTEB leaderboard
</div>

---
layout: section
---

# 1.3
# Ã‰valuation

---
layout: center
---

# 3 Types de MÃ©triques

<div class="grid grid-cols-3 gap-12 mt-16 text-center text-xl">

<div>
<div class="text-5xl mb-6">ğŸ“</div>
<div class="font-bold text-2xl mb-4">Classiques</div>
<div class="space-y-3">
<div>ROUGE</div>
<div>BLEU</div>
<div>PerplexitÃ©</div>
</div>
</div>

<div>
<div class="text-5xl mb-6">ğŸ¤–</div>
<div class="font-bold text-2xl mb-4">LLM-Judge</div>
<div class="space-y-3">
<div>Auto-Ã©valuation</div>
<div>Grilles</div>
<div>Scalable</div>
</div>
</div>

<div>
<div class="text-5xl mb-6">ğŸ¯</div>
<div class="font-bold text-2xl mb-4">Benchmarks</div>
<div class="space-y-3">
<div>MMLU</div>
<div>GPQA</div>
<div>HumanEval</div>
</div>
</div>

</div>

<!--
ğŸ“ Classiques

ROUGE : overlap n-grams â†’ rÃ©sumÃ©s

BLEU : prÃ©cision lexicale â†’ traduction

PerplexitÃ© : prÃ©dictibilitÃ© du texte
â†’ âš ï¸ Ã‰value la forme, pas le sens

ğŸ¤– LLM-as-a-Judge

Un LLM note un autre LLM

CritÃ¨res : exactitude, clartÃ©, utilitÃ©

âœ… Scalable Â· âš ï¸ Biais (position, longueur, auto-faveur)

ğŸ¯ Benchmarks

MMLU â†’ culture & logique

GPQA â†’ raisonnement scientifique

HumanEval â†’ code / logique
â†’ âš ï¸ Saturation (>90% top modÃ¨les)

ğŸ’¡ Ã‰volution : syntaxe â†’ sens â†’ raisonnement
-->

---
layout: two-cols
---

# MÃ©triques Classiques

<div class="text-xl mt-8 space-y-8">

### ROUGE
N-gram overlap

**Usage:** Summarization

### BLEU
Precision-based

**Usage:** Traduction

### PerplexitÃ©
PrÃ©dictibilitÃ©

</div>

::right::

<div class="mt-8 ml-8">

### Limite

<div class="text-2xl mt-16 space-y-8">

âš ï¸ **Syntaxique**
<div class="text-lg text-gray-600">Pas sÃ©mantique</div>

âš ï¸ **Ordre**
<div class="text-lg text-gray-600">Sensible position</div>

âš ï¸ **Sens**
<div class="text-lg text-gray-600">Ne capture pas</div>

</div>

</div>

---
layout: center
---

# LLM-as-a-Judge

<div class="mt-12">

```python {all|2|4-8|10|all}
# Utiliser un LLM pour juger un autre LLM
prompt = f"""
Ã‰value cette rÃ©ponse sur 5 critÃ¨res:
1. Exactitude
2. Pertinence  
3. ClartÃ©
4. ComplÃ©tude
5. UtilitÃ©

Score 1-10 pour chaque.

Question: {question}
RÃ©ponse: {output}
"""

scores = judge_llm(prompt)
```

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
ğŸ“ Notez : LLM-as-a-Judge
</div>

---
layout: center
---

# Limites LLM-Judge

<div class="mt-16 text-2xl space-y-10">

âš ï¸ **Position bias**
<div class="text-lg text-gray-600">PrÃ©fÃ¨re certaines positions</div>

âš ï¸ **Self-enhancement**
<div class="text-lg text-gray-600">Favorise son propre output</div>

âš ï¸ **Verbosity bias**
<div class="text-lg text-gray-600">PrÃ©fÃ¨re rÃ©ponses longues</div>

</div>

<div class="text-center mt-12 text-xl">
ğŸ’¡ Solution : Multiple judges + Calibration humaine
</div>

---
layout: center
---

# MMLU

<div class="text-center mt-12">

<div class="text-2xl mb-8">
Massive Multitask Language Understanding
</div>

<div class="grid grid-cols-3 gap-12 text-xl">

<div>
<div class="text-5xl font-bold text-blue-600">10,000</div>
<div class="mt-2">questions</div>
</div>

<div>
<div class="text-5xl font-bold text-blue-600">57</div>
<div class="mt-2">sujets</div>
</div>

<div>
<div class="text-5xl font-bold text-blue-600">4</div>
<div class="mt-2">niveaux</div>
</div>

</div>

<div class="mt-16 text-lg text-gray-600">
Middle school â†’ PhD
</div>

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-red-500">
âš ï¸ SaturÃ© en 2025 (>90%)
</div>

---
layout: center
---

# GPQA Diamond

<div class="text-center mt-12 space-y-12">

<div class="text-3xl">
Graduate-Level Google-Proof Q&A
</div>

<div class="text-2xl space-y-6">

ğŸ“š **PhD-level** en sciences

ğŸ”¬ Biology Â· Physics Â· Chemistry

âœ… **Expert-validated**

ğŸš« Non publiquement disponible

</div>

<div class="mt-12 text-xl">
Top score : <span class="text-5xl font-bold text-blue-600">87.7%</span> (o3)
</div>

</div>

---
layout: center
---

# HumanEval

<div class="text-center mt-12 space-y-10">

<div class="text-3xl">
Ã‰valuation du Code
</div>

<div class="text-2xl space-y-6">

**164** problÃ¨mes de programmation

Function + Docstring + Tests

Mesure **functional correctness**

</div>

<div class="mt-12">
<div class="text-xl mb-4">Ã‰tat 2025</div>
<div class="text-5xl font-bold text-blue-600">95%+</div>
<div class="text-lg text-gray-600 mt-2">pour top modÃ¨les</div>
</div>

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-red-500">
âš ï¸ Partiellement saturÃ©
</div>

---
layout: center
---

# âš ï¸ La Crise des Benchmarks

<div class="mt-16 text-2xl space-y-10">

**ProblÃ¨me : Saturation**
<div class="text-lg text-gray-600">MMLU >90% Â· HumanEval >95% Â· GSM8K rÃ©solu</div>

**ConsÃ©quence**
<div class="text-lg text-gray-600">Perte de pouvoir discriminant</div>

**Data contamination**
<div class="text-lg text-gray-600">ModÃ¨les entraÃ®nÃ©s sur benchmarks</div>

</div>

<div class="text-center mt-12 text-xl">
ğŸ’¡ Solution : Nouveaux benchmarks dynamiques
</div>

---
layout: center
---

# ğŸ“ Points ClÃ©s du Bloc 1

<div class="grid grid-cols-2 gap-12 mt-12 text-xl">

<div class="space-y-8">

âœ¨ **Transformer (2017)**
<div class="text-base text-gray-600">RÃ©volution architecture</div>

ğŸš€ **3 Enablers**
<div class="text-base text-gray-600">Data Â· Compute Â· Distribution</div>

ğŸ“ˆ **3 Eras**
<div class="text-base text-gray-600">PrÃ©diction â†’ GÃ©nÃ©ration â†’ Raisonnement</div>

</div>

<div class="space-y-8">

ğŸ¯ **SLM vs LLM**
<div class="text-base text-gray-600">Edge vs CapacitÃ©s max</div>

ğŸ§  **Reasoning Models**
<div class="text-base text-gray-600">o3 Â· DeepSeek R1 Â· Claude 4</div>

ğŸ“Š **Ã‰valuation**
<div class="text-base text-gray-600">MÃ©triques Â· Benchmarks Â· Crise</div>

</div>

</div>

---
layout: center
class: text-center
---

# â¸ï¸ Pause
## 15 minutes

---
layout: end
class: text-center
---

# Questions ?

<div class="mt-12 text-xl">

**Prochaine session :**

RAG, CAG, Chain-of-Thought

</div>

<style>
h1 {
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
}
</style>

---
layout: section
---

# Paradigmes d'utilisation

---
layout: center
---

<div class="text-2xl mt-12 space-y-6">

**2.1** RAG â€“ Retrieval-Augmented Generation 

**2.2** CAG â€“ Cache-Augmented Generation â­  

**2.3** Chain-of-Thought & Reasoning 

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
Objectif : mieux exploiter les LLM en contexte rÃ©el
</div>

---
layout: section
---

# 2.1
# Retrieval-Augmented Generation (RAG)

---
layout: center
---

# Pourquoi le RAG ? ğŸ¤”

<div class="mt-16 text-2xl space-y-8">

âŒ Les LLM **oublient** les donnÃ©es rÃ©centes  

âŒ Le contexte est limitÃ© (window de contexte)  

âŒ Les donnÃ©es privÃ©es ne sont **pas** dans le modÃ¨le

</div>

<div class="mt-12 text-2xl">
âœ… <span class="font-bold">RAG = LLM + Base de connaissances</span>  

<span class="text-lg text-gray-600">On "branche" le modÃ¨le sur vos docs, BDD, APIsâ€¦</span>
</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
IdÃ©e clÃ© : sÃ©parer "mÃ©moire longue" (docs) et "raisonnement" (LLM)
</div>

---
layout: center
---

# Vue dâ€™Ensemble

<div class="mt-16 grid grid-cols-3 gap-12 text-center text-xl">

<div>
<div class="text-5xl mb-4">ğŸ”</div>
<div class="font-bold mb-2">1. Retrieve</div>
<div class="text-gray-600">On cherche les passages pertinents</div>
</div>

<div>
<div class="text-5xl mb-4">ğŸ“</div>
<div class="font-bold mb-2">2. Augment</div>
<div class="text-gray-600">On ajoute ces passages au prompt</div>
</div>

<div>
<div class="text-5xl mb-4">ğŸ§ </div>
<div class="font-bold mb-2">3. Generate</div>
<div class="text-gray-600">Le LLM rÃ©pond en sâ€™appuyant dessus</div>
</div>

</div>

<div class="mt-16 text-xl text-gray-600 text-center">
ğŸ’¡ Le modÃ¨le ne â€œsaitâ€ pas tout â†’ il sâ€™appuie sur un contexte injectÃ© dynamiquement
</div>

---
layout: default
---

# Pipeline RAG Complet

<div class="text-xl mt-8 space-y-6">
```mermaid
flowchart LR
  subgraph Indexation["ğŸ§± Data Indexing Pipeline"]
    A[Sources: DB, PDFs, APIs] --> B[PrÃ©paration & Chunking]
    B --> C[Embeddings]
    C --> D[(Vector Store<br/>MongoDB Atlas / pgvector)]
  end

  subgraph RequÃªte["âš™ï¸ Retrieval & Generation Pipeline"]
    Q[User Query] --> QE[Embedding de la requÃªte]
    QE --> R{Recherche vectorielle<br/>dans Vector Store}
    R -->|Top-K documents| G[LLM + Context]
    G --> O[RÃ©ponse vÃ©rifiÃ©e]
  end

  D -.-> R
  style Indexation fill:#0e3b4e,stroke:#00ff99,stroke-width:1px,color:#fff
  style RequÃªte fill:#102f41,stroke:#00ff99,stroke-width:1px,color:#fff

```

</div>

---
layout: center
---

# Ã‰tape 1 : Chunking ğŸ§©

<div class="mt-16 text-2xl space-y-10">

**Pourquoi ?**  
<div class="text-lg text-gray-600">
Les docs sont trop gros â†’ on les dÃ©coupe en "morceaux" exploitables.
</div>

</div>

<div class="grid grid-cols-3 gap-10 mt-12 text-center text-xl">

<div>
<div class="font-bold mb-2">Chunking naÃ¯f</div>
<div class="text-gray-600">Taille fixe (ex : 512 tokens)</div>
</div>

<div>
<div class="font-bold mb-2">Recursive</div>
<div class="text-gray-600">On dÃ©coupe par titre, sous-titre, paragrapheâ€¦</div>
</div>

<div>
<div class="font-bold mb-2">Semantic / Agentic</div>
<div class="text-gray-600">On regroupe par idÃ©e, avec un LLM</div>
</div>

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
Trade-off : morceaux trop petits = perte de contexte Â· trop gros = bruit
</div>


---
layout: center
---

# ğŸ§© Chunking : Attention Ã  lâ€™Overlap

<div class="text-2xl mt-12 space-y-8">

DÃ©couper un document = facile.  
PrÃ©server le sens entre les morceaux = **plus subtil** âš ï¸

</div>


---
layout: default
---

## Prenons le texte:

```
Les bases de donnÃ©es vectorielles comme MongoDB Atlas Vector Search et pgvector
permettent de stocker des embeddings pour le RAG.
Elles sâ€™intÃ¨grent directement dans des environnements de production,
avec rÃ©plication, haute disponibilitÃ© et monitoring.
```

### Sans Overlap

```text
Chunk 1 â†’ "Les bases vectorielles comme MongoDB Atlas Vector Search et pgvector permettent de stocker des embeddings."
Chunk 2 â†’ "Elles sâ€™intÃ¨grent dans des environnements de production avec haute disponibilitÃ©."
````

<div class="mt-3 mb-5 text-xl text-gray-600">
âŒ Le modÃ¨le perd le lien entre *Elles* et *MongoDB Atlas / pgvector*.  
</div>



### Avec Overlap (20 %)

```text
Chunk 1 â†’ "...embeddings pour le RAG. Elles sâ€™intÃ¨grent directement..."
Chunk 2 â†’ "Elles sâ€™intÃ¨grent directement... haute disponibilitÃ©."
```

<div class="mt-3 mb-5 text-xl text-gray-600">
âœ… Contexte prÃ©servÃ© entre les deux segments.  
</div>

---

# âš–ï¸ Choisir le bon Overlap

| Chunk size    | Overlap | Exemple dâ€™usage               |
| ------------- | ------- | ----------------------------- |
| 200 tokens    | 10â€“15 % | Docs techniques courts        |
| 500 tokens    | 20â€“25 % | Articles, rapports            |
| 1 000+ tokens | >25 %   | TrÃ¨s narratif, peu recommandÃ© |

<div class="mt-8 text-xl text-gray-600">
ğŸ’¡ Attention

Trop dâ€™overlap = redondance et coÃ»t.  

Trop peu = perte de cohÃ©rence.
</div>

---

# ğŸ’¬ En rÃ©sumÃ©

<div class="text-2xl mt-12 space-y-6">

**Overlap â‰  duplication**
â†’ câ€™est un *pont sÃ©mantique* entre deux chunks ğŸ§ 

**RÃ¨gle dâ€™or :**
Chunk 200â€“500 tokens avec ~20 % dâ€™overlap
pour un Ã©quilibre entre **cohÃ©rence**, **coÃ»t**, et **pertinence**.

</div>

---
layout: two-cols
---

# Ã‰tape 2 : Embeddings ğŸ“Š

<div class="text-xl mt-8 space-y-6">

- Chaque chunk â†’ **vecteur** (embedding)  
- EncodÃ© dans un espace de dimension 256â€“3072  
- Chunks similaires â†’ vecteurs proches

</div>

<div class="mt-8 text-xl space-y-4">

**Utilisation :**

- Recherche de similaritÃ©  
- Clustering / recommandation  
- Index pour la vector DB

</div>

::right::

<div class="mt-10 ml-8 text-xl space-y-8">

Fonctions typiques :

- `embed(text)` â†’ `[0.12, -0.03, ...]`  
- `cosine_similarity(vec_q, vec_chunk)`  

<div class="text-lg text-gray-600">
Plus la similaritÃ© est haute, plus le chunk est pertinent pour la question.
</div>

</div>

---
layout: center
---

# Ã‰tape 3 : Stockage Vectoriel ğŸ—„ï¸

<div class="mt-14 grid grid-cols-4 gap-8 text-center text-xl">

<div>
<div class="font-bold mb-2">MongoDB</div>
<div class="text-gray-600 text-base">Cloud, scalable</div>
</div>

<div>
<div class="font-bold mb-2">Weaviate</div>
<div class="text-gray-600 text-base">Graph + Vector</div>
</div>

<div>
<div class="font-bold mb-2">ChromaDB</div>
<div class="text-gray-600 text-base">Open-source simple</div>
</div>

<div>
<div class="font-bold mb-2">Qdrant</div>
<div class="text-gray-600 text-base">Perf Â· Rust</div>
</div>


</div>

<div class="mt-16 text-2xl text-center">
ğŸ” Ces bases offrent :<br/>
<span class="text-lg text-gray-600">
Index vectoriels, filtrage par mÃ©tadonnÃ©es, gestion du scale.
</span>
</div>

---
layout: two-cols
---

# Ã‰tape 4 : Retrieval ğŸ”

<div class="text-xl mt-8 space-y-8">

### Similarity Search

- On encode la **question**  
- On cherche les **k** chunks les plus proches  
- Mesure : cosine, dot productâ€¦

### Filtrage

- Par **source**, **date**, **type** de document  
- Ex : "seulement docs internes 2024"

</div>

::right::

<div class="mt-8 ml-8 text-xl space-y-8">

### Hybrid Search

Combiner :

- **BM25** (recherche lexicale)  
- **Vecteurs** (sÃ©mantique)



</div>

<!--
Â« Ici, on parle dâ€™une recherche hybride, câ€™est-Ã -dire une combinaison entre deux approches :

BM25(Best Match 25), la recherche lexicale, trÃ¨s efficace quand on cherche les mÃªmes mots â€” par exemple pour retrouver une rÃ©fÃ©rence ou un code exact.

Et la recherche vectorielle, basÃ©e sur le sens des phrases, donc utile quand on reformule ou quâ€™on ne connaÃ®t pas le bon mot-clÃ©.

En combinant les deux, on obtient le meilleur des deux mondes : la prÃ©cision du lexical et la flexibilitÃ© du sÃ©mantique.

ğŸ’¡ Câ€™est particuliÃ¨rement utile dans des contextes techniques ou avec des noms propres, oÃ¹ les mots exacts comptent.
-->

---
layout: center
---

# ğŸ” Hybrid Search

<div class="grid grid-cols-2 gap-16 items-center mt-16">

<div>

### ğŸ§© Principe

Combiner :
- **BM25** â†’ recherche **lexicale**
- **Vecteurs** â†’ recherche **sÃ©mantique**

<div class="text-lg text-gray-600 mt-6">
ğŸ’¡ Combine prÃ©cision des mots-clÃ©s et comprÃ©hension du sens.
</div>

</div>

<div class="bg-gray-100 rounded-2xl p-6  text-gray-500 shadow-md">
<div class="text-3xl font-bold mb-4">Exemple</div>

<div class="text-lg leading-relaxed space-y-2">
ğŸ§  Question : Â« Comment accÃ©lÃ©rer une requÃªte LLM ? Â»  

ğŸ” <strong>BM25</strong> â†’ trouve â€œLLM cacheâ€  

ğŸ¯ <strong>Vecteur</strong> â†’ trouve â€œprompt cachingâ€, â€œinference cacheâ€
</div>

<div class="mt-6 text-sm text-gray-500">
â†’ RÃ©sultat final : combinaison pondÃ©rÃ©e des deux scores
</div>
</div>

</div>


<div class="absolute  left-1/2 transform -translate-x-1/2 text-lg text-gray-500">
ğŸ“ Utile quand le vocabulaire est technique ou les noms propres critiques
</div>

---
layout: center
---

# Ã‰tape 5 : Reranking ğŸ¯


<div class="mt-2 text-2xl space-y-10">

**ProblÃ¨me :**  
La recherche vectorielle ramÃ¨ne des chunks Â« Ã  peu prÃ¨s Â» pertinents.

**Solution :**  
```mermaid
flowchart LR
  Q[ğŸ§‘â€ğŸ’» User Query] --> V[ğŸ” Vector Search<br/>Top-20 rÃ©sultats]
  V --> R[ğŸ¤– Reranker LLM spÃ©cialisÃ©]
  R -->|Scores de pertinence| S[ğŸ“Š Top-5 meilleurs passages]
  S --> L[ğŸ§  LLM avec contexte filtrÃ©]
```
<div class="text-xl">
ModÃ¨le de <strong>reranking</strong> (Cohere, Jina, etc.) qui :


- relit la <strong>question + chaque chunk</strong> 
 
- donne un <strong>score de pertinence</strong> 

- rÃ©ordonne et garde les meilleurs
</div>

</div>


<div class="mt-8 text-xl text-gray-600 text-center">
On passe de "pertinent globalement" Ã  "ultra ciblÃ© pour cette question".
</div>

---
layout: center
---

# ğŸ§© Exemple : Reranking en action

<div class="grid grid-cols-2 gap-16 items-center mt-12">

<div class="space-y-6 text-xl">

### ğŸ§  Question
> "Quels sont les avantages du cache sÃ©mantique dans un RAG ?"

### ğŸ” Recherche initiale (vectorielle)
1ï¸âƒ£ "Le cache rÃ©duit la latence."  
2ï¸âƒ£ "Les embeddings sont stockÃ©s dans ChromaDB."  
3ï¸âƒ£ "Le cache sÃ©mantique Ã©vite les requÃªtes redondantes."  
4ï¸âƒ£ "Les chunks sont dÃ©coupÃ©s en 512 tokens."



</div>

<div class="bg-gray-100 text-gray-500 rounded-2xl p-6 shadow-md">

### ğŸ¤– Reranker (ex : Cohere Rerank v3)
Re-score chaque passage selon la **pertinence exacte Ã  la question** :

| Passage | Score (0-1) |
|----------|-------------|
| "Le cache sÃ©mantique Ã©vite les requÃªtes redondantes." | ğŸŸ¢ **0.95** |
| "Le cache rÃ©duit la latence." | ğŸŸ¢ **0.82** |
| "Les embeddings sont stockÃ©s dans ChromaDB." | âšª 0.21 |
| "Les chunks sont dÃ©coupÃ©s en 512 tokens." | âšª 0.10 |

<div class="mt-6 text-lg font-semibold text-center">
ğŸ¯ RÃ©sultat : le contexte final garde les 2 premiers passages â†’ plus prÃ©cis, moins de bruit.
</div>

</div>

</div>

<!--
ğŸ“ Le reranking affine la recherche avant lâ€™appel au LLM.
-->

---
layout: center
---

# RAG AvancÃ© 

<div class="mt-16 grid grid-cols-2 gap-10 text-xl">

<div class="space-y-6">

### Self-RAG
Le LLM **sâ€™auto-contrÃ´le** :  
<span class="text-gray-600 text-base">â€œAi-je assez de contexte ? Dois-je relancer une recherche ?â€</span>

### Corrective RAG
Corrige les erreurs :  
<span class="text-gray-600 text-base">post-vÃ©rification des rÃ©ponses et "repair" si nÃ©cessaire.</span>

</div>

<div class="space-y-6">

### Adaptive RAG
StratÃ©gie **dynamique** :  
<span class="text-gray-600 text-base">choix du nombre de documents, du type de rechercheâ€¦</span>

### Agentic RAG
Des **agents** pilotent le RAG :  
<span class="text-gray-600 text-base">boucles de rÃ©flexion, multi-outils, workflows complexes.</span>

</div>

</div>

---
layout: center
---

# Ã‰valuation du RAG ğŸ“

<div class="mt-16 text-2xl space-y-10">

### RAGAS & Co

<div class="text-lg text-gray-600">
Frameworks qui mesurent :
</div>


- **Context Relevance** : contexte bien choisi ?  
- **Answer Correctness** : rÃ©ponse juste ?  
- **Faithfulness** : la rÃ©ponse sâ€™appuie-t-elle sur le contexte ?


</div>

<div class="mt-10 text-xl text-center">
ğŸ’¡ IdÃ©e clÃ© : on nâ€™Ã©value plus seulement le LLM, mais le <strong>pipeline complet</strong>.
</div>

---
layout: center
---

# ğŸ§© SynthÃ¨se 2.1 â€“ RAG

<div class="mt-14 grid grid-cols-2 gap-10 text-xl">

<div class="space-y-6">

âœ… SÃ©parer **raisonnement** (LLM) et **connaissance** (docs)  
âœ… Pipeline : ingestion â†’ chunking â†’ embeddings â†’ retrieval â†’ gÃ©nÃ©ration  
âœ… Vector DB = cÅ“ur de la stack RAG

</div>

<div class="space-y-6">

âš ï¸ Chunking : trouver le bon **granularitÃ©**  
âš ï¸ Retrieval seul ne suffit pas â†’ **reranking** recommandÃ©  
âš ï¸ Importance dâ€™**Ã©valuer** le pipeline (RAGAS, faithfulnessâ€¦)

</div>

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
Next : 2.2 â€“ Cache-Augmented Generation (CAG) â­
</div>


---
layout: section
---

# 2.2
# Cache-Augmented Generation (CAG)

---
layout: center
---

# âš¡ Cache-Augmented Generation
## RÃ©utiliser intelligemment au lieu de recalculer

<div class="mt-12 text-2xl space-y-8">

ğŸ’¡ **ProblÃ¨me :**  
Chaque requÃªte Ã  un LLM coÃ»te du **temps**, des **tokens** et donc de lâ€™**argent**.

ğŸ§  **IdÃ©e du CAG :**  
Quand une requÃªte ou un contexte est **similaire Ã  un dÃ©jÃ  traitÃ©**,  
on **rÃ©utilise** une partie ou la totalitÃ© du rÃ©sultat au lieu de tout recalculer.

</div>

---
layout: center
---

# ğŸ” Pourquoi le Caching est essentiel en production

<div class="grid grid-cols-3 gap-10 mt-16 text-center text-xl">

<div>
<div class="text-5xl mb-4">ğŸ’°</div>
<div class="font-bold mb-2">RÃ©duction des coÃ»ts</div>
<div class="text-gray-600">Moins dâ€™appels LLM â†’ moins de tokens â†’ -70 Ã  -90 %</div>
</div>

<div>
<div class="text-5xl mb-4">âš¡</div>
<div class="font-bold mb-2">Gain de latence</div>
<div class="text-gray-600">RÃ©ponse immÃ©diate si cache hit</div>
</div>

<div>
<div class="text-5xl mb-4">ğŸ“ˆ</div>
<div class="font-bold mb-2">ScalabilitÃ©</div>
<div class="text-gray-600">Les serveurs gÃ¨rent plus dâ€™utilisateurs avec les mÃªmes ressources</div>
</div>

</div>

---
layout: center
---

# ğŸ§© RAG vs CAG

| CaractÃ©ristique | **RAG** | **CAG** |
|-----------------|:------:|:------:|
| Objectif | AmÃ©liorer la **connaissance** | AmÃ©liorer la **performance** |
| Source externe | Oui (vector DB) | Non (cache interne) |
| DonnÃ©es injectÃ©es | Contexte documentaire | Contexte dÃ©jÃ  traitÃ© |
| Type de gain | Pertinence | Latence / coÃ»t |
| ComplÃ©mentaire ? | âœ… Oui, RAG â†’ CAG combinables |

<div class="mt-8 text-xl text-gray-600">
ğŸ’¬ CAG â‰  RAG : le cache accÃ©lÃ¨re le raisonnement, le RAG amÃ©liore la connaissance.
</div>

---
layout: center
---

# âš™ï¸ Types de caches dans le CAG

<div class="grid grid-cols-2 gap-12 mt-16 text-xl">

<div class="space-y-2">

### ğŸ§  KV Cache (Key-Value)
- MÃ©morise les **Ã©tats dâ€™attention** dans les Transformers  
- UtilisÃ© lors de lâ€™**infÃ©rence streaming**  
- Permet de gÃ©nÃ©rer plus vite la suite dâ€™un texte  
### ğŸ’¬ Prompt Caching
- Stocke des prompts ou requÃªtes dÃ©jÃ  vues  
- Exemple : Anthropic, OpenAI, Google  
- Hash du prompt â†’ rÃ©ponse rÃ©utilisable  

</div>

<div class="space-y-2">

### ğŸ” Semantic Caching
- Recherche de similaritÃ© sur les prompts  
- Si une requÃªte est **proche sÃ©mantiquement** dâ€™une prÃ©cÃ©dente â†’ rÃ©utilisation partielle  
- ImplÃ©mentÃ© dans **LiteLLM** et **LangChain**

### ğŸ§© Context Caching
- RÃ©utilisation de blocs contextuels rÃ©currents  
- Ex : â€œsystÃ¨me promptâ€ constant, ou contexte RAG stable  

</div>

</div>

---
layout: center
---

# ğŸ”¬ Illustration : Semantic Cache Flow

```mermaid
flowchart LR
  Q[ğŸ§‘â€ğŸ’» Nouvelle requÃªte] --> H{ğŸ” Hash / Embedding<br/>dÃ©jÃ  existant ?}
  H -->|Oui| R[ğŸ’¾ Retourne rÃ©ponse cache]
  H -->|Non| L[ğŸ¤– Appel LLM]
  L --> S[ğŸª£ Stocke embedding + rÃ©ponse<br/>dans cache sÃ©mantique]
  S --> E[âœ… RÃ©ponse utilisateur]
````

<div class="text-xl text-gray-600 mt-8">
ğŸ’¡ Un cache sÃ©mantique fonctionne comme une **mini base vectorielle interne**.
</div>

---

# ğŸ§  KV Cache â€“ Niveau modÃ¨le

<div class="text-xl mt-8 space-y-6">

* Lors de lâ€™infÃ©rence, le LLM garde en mÃ©moire les clÃ©s et valeurs dâ€™attention.
* Ã€ chaque token gÃ©nÃ©rÃ©, il **rÃ©utilise les clÃ©s/valeurs dÃ©jÃ  calculÃ©es**
  plutÃ´t que de recalculer tout le contexte.

ğŸ‘‰ RÃ©sultat : gÃ©nÃ©ration 2â€“4Ã— plus rapide.

</div>

::right::

```mermaid
flowchart LR
  A[Input Tokens] --> B[Transformer Layer]
  B -->|K,V stockÃ©s| C[(KV Cache)]
  C --> D[Suivant Tokens<br/>rÃ©utilisent les K,V]
  D --> E[Sortie plus rapide]
```

<div class="text-sm text-gray-500 mt-6">
ImplÃ©mentÃ© nativement dans <b>vLLM, TGI, TensorRT-LLM, Ollama</b>.
</div>

---

# ğŸ’¾ Exemple dâ€™implÃ©mentation

## (Prompt / Semantic Cache avec LangChain)

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# 1ï¸âƒ£ Cache mÃ©moire simple
set_llm_cache(InMemoryCache())

# 2ï¸âƒ£ Cache vectoriel (sÃ©mantique)
embeddings = OpenAIEmbeddings()
vector_cache = Chroma(embedding_function=embeddings)

# 3ï¸âƒ£ VÃ©rification avant requÃªte
if vector_cache.similarity_search(query, k=1)[0].score > 0.9:
    return cached_response
else:
    response = llm.invoke(query)
    vector_cache.add_texts([query], metadatas=[response])
```

<div class="text-xl mt-6 text-gray-600">
ğŸ’¡ En prod : remplacer InMemoryCache par Redis, Milvus, ou MongoDB Atlas Vector Search.
</div>

---

# âš™ï¸ Politiques dâ€™invalidation

| Type de stratÃ©gie                | Description                                | Usage                |
| -------------------------------- | ------------------------------------------ | -------------------- |
| â± **TTL (Time-to-Live)**         | Expire aprÃ¨s une durÃ©e donnÃ©e              | Questions volatiles  |
| ğŸ§¹ **LRU (Least Recently Used)** | Supprime les plus anciens hits             | Cache mÃ©moire limitÃ© |
| ğŸ§  **Heuristique / ML**          | BasÃ©e sur frÃ©quence ou score de similaritÃ© | Cache intelligent    |

<div class="mt-8 text-xl text-gray-600">
ğŸ’¬ Toujours Ã©quilibrer performance vs fraÃ®cheur des rÃ©sultats.
</div>

---

# ğŸ§® Impact mesurÃ©

| Type de cache  | Latence   | CoÃ»t tokens | Exemple                        |
| -------------- | --------- | ----------- | ------------------------------ |
| KV Cache       | â†“ 50â€“90 % | â€”           | vLLM, TGI                      |
| Prompt Cache   | â†“ 70 %    | â†“ 70â€“90 %   | Anthropic, OpenAI              |
| Semantic Cache | â†“ 40â€“70 % | â†“ 30â€“80 %   | LiteLLM, LangChain             |
| Context Cache  | â†“ 30â€“50 % | â†“ 20â€“60 %   | Google Gemini, OpenAI o-series |

<div class="mt-8 text-xl text-gray-600">
ğŸ’¡ Cumulable : KV Cache + Semantic Cache = pipeline ultra-rapide âš¡
</div>

---

# ğŸ—ï¸ IntÃ©gration en production

<div class="grid grid-cols-2 gap-12 mt-12 text-xl">


âœ… **LangChain / LiteLLM** : gestion native du cache sÃ©mantique  
âœ… **FastAPI** : endpoint REST avec Redis ou MongoDB comme cache  
âœ… **vLLM / TensorRT-LLM** : KV Cache bas-niveau  
âœ… **Prometheus / Grafana** : suivi cache hit-rate et latence


âš ï¸ **Invalidation obligatoire** aprÃ¨s mises Ã  jour de donnÃ©es  
âš ï¸ **Versionner les prompts** (hash prompt + model id)  
âš ï¸ **SÃ©curiser les stores** (auth, chiffrement, RBAC)  
âš ï¸ **Ã‰viter fuite de donnÃ©es** dans logs de cache


</div>

---

# ğŸ’¬ En rÃ©sumÃ© â€“ Cache Augmented Generation

<div class="grid grid-cols-2 gap-12 mt-12 text-xl">


âœ… RÃ©utiliser au lieu de recalculer  
âœ… Baisse latence & coÃ»ts drastique  
âœ… Compatible RAG / production  
âœ… Plusieurs niveaux : KV, Prompt, Semantic, Context


âš ï¸ GÃ©rer invalidation et sÃ©curitÃ©  
âš ï¸ Ã‰viter stale data et collisions  
âš ï¸ Surveiller hit-rate %  
âš ï¸ Documenter politiques de cache



</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
Next â†’ 2.3 : Chain-of-Thought & Reasoning ğŸ§ 
</div>

---
layout: section
---

# 2.3
# Chain-of-Thought & Reasoning

---
layout: center
---

# ğŸ§  Chain-of-Thought (CoT)
## Apprendre Ã  faire â€œpenserâ€ les modÃ¨les

<div class="mt-12 text-2xl space-y-8">

Les LLM ne â€œraisonnentâ€ pas vraiment :  
ils **gÃ©nÃ¨rent du texte** en suivant des corrÃ©lations.  

Mais on peut leur **enseigner Ã  raisonner explicitement**,  
en leur demandant de **montrer leurs Ã©tapes de pensÃ©e**.

ğŸ’¡ Câ€™est le **Chain-of-Thought (CoT)**.

</div>

---
layout: center
---

# ğŸ”— Principe du Chain-of-Thought

```text
Question : Si un train part Ã  10h de Paris Ã  100 km/h et un autre part Ã  11h de Lyon Ã  120 km/h, quand se croisent-ils ?

RÃ©ponse (sans CoT) : 12h47  
RÃ©ponse (avec CoT) :
- Distance Parisâ€“Lyon = 460 km
- En 1h, le premier train parcourt 100 km â†’ reste 360 km
- Vitesse relative = 100 + 120 = 220 km/h
- Temps pour se croiser = 360 / 220 â‰ˆ 1.63 h
â†’ RÃ©ponse finale : 12h38
````

<div class="mt-8 text-xl text-gray-600">
ğŸ’¬ CoT = on ne veut pas juste la bonne rÃ©ponse, on veut le raisonnement qui y mÃ¨ne.
</div>

---
layout: two-cols
---

# ğŸ§© Pourquoi Ã§a marche
<div class="text-xl mt-8 space-y-4">

âœ… Permet de **structurer la rÃ©flexion** du modÃ¨le

âœ… RÃ©duit les **erreurs logiques** et les hallucinations

âœ… AmÃ©liore la **robustesse aux reformulations**

âœ… Utile pour :

* RÃ©solution de problÃ¨mes
* Calculs multi-Ã©tapes
* Raisonnement logique ou causal

</div>
::right::

```mermaid
flowchart TD
  Q[Question utilisateur] --> R1[Ã‰tape 1 : Analyse du problÃ¨me]
  R1 --> R2[Ã‰tape 2 : DÃ©duction]
  R2 --> R3[Ã‰tape 3 : VÃ©rification]
  R3 --> A[âœ… RÃ©ponse finale]
```

<div class="text-sm text-gray-500 mt-4">
ğŸ§  Le modÃ¨le â€œsimuleâ€ une pensÃ©e structurÃ©e avant de rÃ©pondre.
</div>

---
layout: default
---

# ğŸ§® Zero-shot / Few-shot / Many-shot

| Type          | Exemple                                | Usage                      |
| ------------- | -------------------------------------- | -------------------------- |
| **Zero-shot** | â€œExplique ta rÃ©ponse Ã©tape par Ã©tape.â€ | Simple, efficace           |
| **Few-shot**  | Ajout dâ€™exemples de raisonnement       | Enseignement par imitation |
| **Many-shot** | +20 exemples contextuels               | Fine-tuning implicite      |

<div class="mt-8 text-xl text-gray-600">
ğŸ’¡ Le Few-shot CoT est le plus stable et le plus utilisÃ© en production (OpenAI, Anthropic).
</div>

---
layout: default
---

# ğŸ§  Self-Consistency

<div class="mt-12 text-2xl space-y-8">

Les modÃ¨les peuvent se contredire dâ€™un run Ã  lâ€™autre.
Pour pallier Ã§a, on fait plusieurs CoT et on vote ğŸ—³ï¸

â†’ Le modÃ¨le **explore plusieurs chemins de pensÃ©e**,
puis **choisit la rÃ©ponse la plus frÃ©quente ou la plus cohÃ©rente.**

</div>

<div class="text-center text-xl text-gray-600 mt-8">
ğŸ’¬ Technique utilisÃ©e dans les modÃ¨les de raisonnement comme DeepSeek R1 ou OpenAI o3.
</div>

---
layout: two-cols
---
# ğŸ§¬ Tree-of-Thoughts (ToT)

<div class="text-xl mt-8 space-y-6">

* Au lieu dâ€™une seule chaÃ®ne linÃ©aire de raisonnement,
  le modÃ¨le explore **plusieurs branches possibles**.
* Chaque Ã©tape ouvre des â€œhypothÃ¨sesâ€
  â†’ on garde les meilleures, on rejette les autres.
* Approche inspirÃ©e des **search trees** (DFS, BFS).

</div>

::right::

```mermaid
graph TD
  Q(Question) --> A1[IdÃ©e 1]
  Q --> A2[IdÃ©e 2]
  A1 --> B1[DÃ©duction 1]
  A1 --> B2[DÃ©duction 2]
  A2 --> B3[DÃ©duction 3]
  B2 --> C1[RÃ©ponse finale ğŸ]
```

<div class="text-sm text-gray-500 mt-6">
ğŸŒ³ Tree-of-Thoughts = raisonnement exploratoire.
</div>

---
layout: two-cols
---

# ğŸ§¬ Tree-of-Thoughts (ToT)

<div class="text-xl mt-8 space-y-6">

* Au lieu dâ€™une seule chaÃ®ne linÃ©aire de raisonnement,
  le modÃ¨le explore **plusieurs branches possibles**.
* Chaque Ã©tape ouvre des â€œhypothÃ¨sesâ€
  â†’ on garde les meilleures, on rejette les autres.
* Approche inspirÃ©e des **search trees** (DFS, BFS).

</div>

::right::

```mermaid
graph TD
  Q(Question) --> A1[IdÃ©e 1]
  Q --> A2[IdÃ©e 2]
  A1 --> B1[DÃ©duction 1]
  A1 --> B2[DÃ©duction 2]
  A2 --> B3[DÃ©duction 3]
  B2 --> C1[RÃ©ponse finale ğŸ]
```

<div class="text-sm text-gray-500 mt-6">
ğŸŒ³ Tree-of-Thoughts = raisonnement exploratoire.
</div>

---
layout: center
---

# ğŸ” Structuration et formats de sortie

| Format               | Exemple                                   | Utilisation              |
| -------------------- | ----------------------------------------- | ------------------------ |
| **JSON mode**        | `{ "reasoning": "...", "answer": "..." }` | API, intÃ©gration fiable  |
| **Function calling** | `function(solve_problem)`                 | Interaction avec backend |
| **ReAct pattern**    | â€œThink â†’ Act â†’ Observeâ€                   | Agents autonomes         |
| **Plan-and-Execute** | â€œPlan tasks â†’ Execute sub-goalsâ€          | Multi-step workflows     |

<div class="mt-8 text-xl text-gray-600">
ğŸ’¡ Lâ€™objectif : **rendre le raisonnement exploitable** par le code.
</div>

---
layout: default
---

# ğŸ¤– ModÃ¨les de Raisonnement

<div class="grid grid-cols-2 gap-12 mt-16 text-2xl">

<div class="space-y-6">

ğŸ§  **OpenAI o3 / o1-mini**  
<div class="text-lg text-gray-600">
BasÃ©s sur reasoning traces Â· 87.7% GPQA Diamond
</div>

ğŸ§© **DeepSeek R1**

<div class="text-lg text-gray-600">
Open-source Â· 671B params Â· RL-only Â· raisonnement pur
</div>
</div>

<div class="space-y-6">

âš™ï¸ **Claude 4 (Opus)**  
<div class="text-lg text-gray-600">
Raisonnement mixte + outils externes
</div>

ğŸ§® **Gemini 2.5 Pro / Flash**

<div class="text-lg text-gray-600">
OptimisÃ©s pour rÃ©flexion rapide et multi-modale
</div>
</div>

</div>

<div class="mt-10 text-xl text-gray-600">
ğŸ’¡ Tous exploitent le CoT, Self-Consistency, et traces internes de raisonnement.
</div>

---
layout: default
---

# ğŸ§© CoT et Agents

<div class="text-2xl mt-12 space-y-8">

Les **agents** sont des systÃ¨mes basÃ©s sur LLM
qui utilisent le raisonnement **pour dÃ©cider dâ€™agir**.

â†’ Patterns frÃ©quents :

* **ReAct** : penser â†’ agir â†’ observer
* **Plan-and-Execute** : plan global puis exÃ©cution pas Ã  pas
* **Reflexion / Self-RAG** : raisonnement auto-correctif

</div>

<div class="text-xl mt-8 text-gray-600">
ğŸ’¡ Le CoT devient la â€œmÃ©moire de travailâ€ dâ€™un agent autonome.
</div>

---
layout: default
---

# ğŸ¯ En rÃ©sumÃ© â€“ Chain-of-Thought & Reasoning

<div class="grid grid-cols-2 gap-12 mt-12 text-xl">

<div class="space-y-6">

âœ… CoT = raisonnement explicite (montrer les Ã©tapes)  
âœ… Few-shot & self-consistency â†’ stabilitÃ©  
âœ… Tree-of-Thoughts â†’ raisonnement exploratoire  
âœ… Formats structurÃ©s (JSON, ReAct, Plan-Execute)
</div>

<div class="space-y-6">

âš ï¸ CoÃ»t plus Ã©levÃ© en tokens  
âš ï¸ Risque de fuite de raisonnement (â€œshow your workâ€)  
âš ï¸ NÃ©cessite Ã©valuation spÃ©cifique (logic accuracy, coherence)
</div>

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
Next â†’ Pause â˜• avant Bloc 3 : Enablers & Protocoles
</div>

---
layout: section
---

# 3.1
# Frameworks d'Orchestration

---
layout: center
---

# âš™ï¸ Pourquoi un framework dâ€™orchestration ?

<div class="mt-12 text-2xl space-y-8">

ğŸ’¡ Dans un projet IA, il ne suffit pas dâ€™appeler un LLM.  
Il faut **enchaÃ®ner** : chargement des donnÃ©es, vectorisation, requÃªtes, cache, appels modÃ¨le, post-traitement, logsâ€¦

Ces pipelines deviennent vite complexes.  
Les frameworks dâ€™orchestration permettent de les **structurer, tester et monitorer**.

</div>

<div class="mt-10 text-xl text-gray-600">
ğŸ¯ Objectif : industrialiser la gÃ©nÃ©ration, le RAG et les agents.
</div>

---
layout: center
---

# ğŸ§© Ã‰cosystÃ¨me 2025

<div class="grid grid-cols-3 gap-8 mt-16 text-center text-xl">

<div>
<div class="text-5xl mb-4">ğŸ”—</div>
<div class="font-bold mb-2">LangChain</div>
<div class="text-gray-600">Le plus populaire Â· Chains & Graphs</div>
</div>

<div>
<div class="text-5xl mb-4">ğŸ“š</div>
<div class="font-bold mb-2">LlamaIndex</div>
<div class="text-gray-600">OrientÃ© Data / RAG avancÃ©</div>
</div>

<div>
<div class="text-5xl mb-4">âš¡</div>
<div class="font-bold mb-2">LiteLLM</div>
<div class="text-gray-600">API universelle pour tous les LLMs</div>
</div>

<div>
<div class="text-5xl mb-4">ğŸ¤</div>
<div class="font-bold mb-2">AutoGen / CrewAI</div>
<div class="text-gray-600">Multi-agents collaboratifs</div>
</div>

<div>
<div class="text-5xl mb-4">ğŸ§ </div>
<div class="font-bold mb-2">Semantic Kernel</div>
<div class="text-gray-600">Orchestration Microsoft / .NET</div>
</div>

</div>

---
layout: two-cols
---

# ğŸ”— LangChain (Python / JS)

<div class="text-xl mt-8 space-y-6">

- Le plus complet et documentÃ©  
- Concept clÃ© : **Chain** â†’ suite dâ€™Ã©tapes modulaires  
- Nouvelle syntaxe : **LCEL (LangChain Expression Language)**  
- Supporte :
  - LLMs, VectorStores, Tools, Agents  
  - Caching, Reranking, Evaluation (LangSmith)
- IntÃ©gration avec FastAPI, Airflow, MongoDB, Postgres

</div>

::right::

```mermaid
flowchart TD
  D[ğŸ“„ Documents] --> E[ğŸ” Embeddings]
  E --> V[(Vector Store)]
  V --> R[ğŸ” Retrieval Chain]
  R --> L[ğŸ¤– LLMChain]
  L --> O[âœ… Output]
````

<div class="text-sm text-gray-500 mt-4">
LangChain structure le pipeline complet : ingestion â†’ recherche â†’ gÃ©nÃ©ration.
</div>

---
layout: two-cols
---

# ğŸ“š LlamaIndex

<div class="text-xl mt-8 space-y-6">

* Ex-GPT Index : orientÃ© **Data RAG**
* Se concentre sur la **gestion de documents, index et requÃªtes complexes**
* Supporte plusieurs backends vectoriels : Chroma, MongoDB Atlas, Weaviate, Qdrant, pgvector
* IntÃ¨gre le **graph RAG** (sÃ©mantique hiÃ©rarchique)
* TrÃ¨s utilisÃ© pour projets **multi-source** ou **data warehouse**

</div>

::right::

```mermaid
flowchart LR
  A[PDF / SQL / API] --> B[Index Builders]
  B --> C[(Graph Index)]
  C --> D[Retriever]
  D --> L[LLM]
```

<div class="text-sm text-gray-500 mt-4">
ğŸ’¡ LlamaIndex = colle entre tes donnÃ©es et ton modÃ¨le.
</div>

---
layout: two-cols
---

# âš¡ LiteLLM

<div class="text-xl mt-8 space-y-6">

* API unique compatible avec **100+ LLMs** (OpenAI, Anthropic, Mistral, Ollama, Azureâ€¦)
* Standardise le format OpenAI (`chat.completions.create`)
* Simplifie :

  * Le **fallback** entre fournisseurs
  * Le **load balancing**
  * Le **tracking coÃ»ts**
  * Le **logging et caching**

</div>

::right::

```mermaid
flowchart TD
  A[App / API] --> L[LiteLLM Gateway]
  L -->|OpenAI| O[GPT-4]
  L -->|Anthropic| C[Claude 4]
  L -->|Mistral| M[Mistral Large]
  L -->|Local| X[Ollama / LM Studio]
```

<div class="text-sm text-gray-500 mt-6">
ğŸ’¡ LiteLLM = â€œle Stripe des APIs LLMâ€ : unifie lâ€™accÃ¨s, rÃ©duit la friction.
</div>

---
layout: two-cols
---

# ğŸ¤ AutoGen / CrewAI (Agents)

<div class="text-xl mt-8 space-y-6">

* **AutoGen** (Microsoft) : agents communicants avec rÃ´les (user, assistant, critic)
* **CrewAI** : framework multi-agents open-source (Python)
* Permet la coordination de plusieurs LLMs dans une tÃ¢che :

  * Planification â†’ exÃ©cution â†’ validation
* Supporte la persistance, mÃ©moire et supervision humaine

</div>

::right::

```mermaid
flowchart LR
  U[ğŸ§‘â€ğŸ’» Utilisateur] --> A1[Agent 1 : Plan]
  A1 --> A2[Agent 2 : ExÃ©cute]
  A2 --> A3[Agent 3 : VÃ©rifie]
  A3 --> R[âœ… RÃ©sultat final]
```

<div class="text-sm text-gray-500 mt-6">
Agents = plusieurs cerveaux LLM coordonnÃ©s pour rÃ©soudre un problÃ¨me.
</div>

---
layout: two-cols
---

# ğŸ§  Semantic Kernel (Microsoft)

<div class="text-xl mt-8 space-y-6">

* Framework open-source .NET / Python
* Permet de combiner prompts, outils, mÃ©moire et plans
* Sâ€™intÃ¨gre parfaitement Ã  Azure OpenAI, Microsoft Fabric, et Graph API
* Fort accent sur **sÃ©curitÃ©, monitoring et intÃ©gration entreprise**

</div>

::right::

```mermaid
flowchart LR
  U[User Request] --> P[Planner]
  P --> F[Functions - Tools]
  F --> M[Memory Store]
  M --> L[LLM Execution]
  L --> R[Response]
```

<div class="text-sm text-gray-500 mt-4">
ğŸ§© Semantic Kernel = orchestrateur pour environnements Microsoft/enterprise.
</div>

---
layout: center
---

# âš–ï¸ Comparatif rapide

| Framework            | Points forts                     | Cas dâ€™usage typique       |
| -------------------- | -------------------------------- | ------------------------- |
| **LangChain**        | Ã‰cosystÃ¨me riche, standard open  | RAG / agents gÃ©nÃ©ralistes |
| **LlamaIndex**       | Gestion fine des donnÃ©es & index | Data-centric RAG          |
| **LiteLLM**          | Gateway unifiÃ©e pour 100+ LLMs   | Production multi-LLM      |
| **AutoGen / CrewAI** | Coordination multi-agents        | Automatisation complexe   |
| **Semantic Kernel**  | IntÃ©gration Azure / entreprise   | Environnements corporate  |

<div class="mt-10 text-xl text-gray-600">
ğŸ’¬ En pratique : souvent, **LangChain + LiteLLM + LlamaIndex** sont utilisÃ©s ensemble.
</div>

---
layout: center
---

# ğŸ“ En rÃ©sumÃ© â€“ Frameworks dâ€™Orchestration

<div class="grid grid-cols-2 gap-12 mt-12 text-xl">

<div class="space-y-6">

âœ… Structurer les pipelines IA (RAG, CAG, Reasoning)  
âœ… Simplifier les intÃ©grations multi-LLM  
âœ… Ajouter monitoring, cache et logs  
âœ… Base pour lâ€™intÃ©gration de MCP et A2A (prochaines sections)
</div>

<div class="space-y-6">

âš ï¸ Attention Ã  la complexitÃ© des abstractions  
âš ï¸ Certaines libs Ã©voluent trÃ¨s vite â†’ veille nÃ©cessaire  
âš ï¸ Toujours Ã©valuer latence / coÃ»t / maintenabilitÃ©
</div>

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
Next â†’ 3.2 : Model Context Protocol (MCP) ğŸ”Œ
</div>
Excellent ğŸ‘Œ
On passe donc Ã  **3.2 â€“ Model Context Protocol (MCP)**, la sous-partie la plus importante du **Bloc 3**, car elle relie **tout le reste du cours** : câ€™est **la normalisation des intÃ©grations IA**.

Ce module est clÃ© pour comprendre comment les LLMs modernes (Claude, OpenAI o-series, Google Gemini 2.5â€¦) se **branchent Ã  des outils, donnÃ©es et environnements externes** de maniÃ¨re standardisÃ©e â€” et sÃ©curisÃ©e.


---
layout: section
---

# 3.2
# Model Context Protocol (MCP)

---
layout: center
---

# ğŸ”Œ Model Context Protocol (MCP)
## Le â€œUSB-Câ€ universel pour connecter les IA

<div class="mt-12 text-2xl space-y-8">

ğŸ’¡ MCP est un **standard ouvert** crÃ©Ã© en 2024  
pour permettre aux modÃ¨les et agents IA de  
**se connecter Ã  nâ€™importe quelle source de donnÃ©es ou outil**  
de maniÃ¨re **unifiÃ©e et sÃ©curisÃ©e**.

</div>

<div class="mt-10 text-xl text-gray-600">
ConÃ§u initialement par Anthropic Â· AdoptÃ© en 2025 par OpenAI, Google, AWS, etc.
</div>

---
layout: center
---

# ğŸ§© Pourquoi MCP est nÃ©

<div class="text-2xl mt-12 space-y-8">

Avant MCP, chaque acteur (OpenAI, Anthropic, Google, etc.)  
avait son propre format dâ€™outils et dâ€™intÃ©grations.  

â¡ï¸ RÃ©sultat : un **problÃ¨me MÃ—N**  
> chaque modÃ¨le devait gÃ©rer chaque API sÃ©parÃ©ment.

</div>

<div class="text-center mt-8 text-xl text-gray-600">
ğŸ’¬ MCP standardise la communication entre modÃ¨les, outils et donnÃ©es.
</div>

---
layout: center
---

# âš™ï¸ Architecture gÃ©nÃ©rale

```mermaid
flowchart LR
  A[ğŸ¤– MCP Client<br/>Agent / LLM] --> B[ğŸ”Œ MCP Server<br/>Outil / Source de donnÃ©es]
  B --> A
  A --> U[ğŸ§‘â€ğŸ’» Utilisateur / Application]
  B --> D[(ğŸ“‚ Base de donnÃ©es / API / Fichier)]
````

<div class="text-xl mt-6 text-gray-600">

Le LLM dialogue avec des serveurs MCP via un protocole commun (JSON-RPC 2.0).  
Chaque serveur expose ses **tools**, **resources**, **prompts** et **sampling**.
</div>

---
layout: two-cols
---

# ğŸ§  Les 4 capacitÃ©s MCP

<div class="text-xl mt-8 space-y-6">

### 1ï¸âƒ£ **Resources**

AccÃ¨s aux donnÃ©es (fichiers, APIs, BDD)
â†’ lecture, filtrage, mÃ©tadonnÃ©es

### 2ï¸âƒ£ **Tools**

Actions exÃ©cutables par le modÃ¨le
â†’ â€œcall toolâ€ universel (Ã©quivalent Ã  function calling)

### 3ï¸âƒ£ **Prompts**

Templates prÃ©-dÃ©finis exposÃ©s par le serveur
â†’ rÃ©utilisables par diffÃ©rents agents

### 4ï¸âƒ£ **Sampling**

Gestion dÃ©lÃ©guÃ©e du LLM (stream, multi-modÃ¨les)

</div>

::right::

```mermaid
flowchart TD
  S[(MCP Server)] --> R[Resources]
  S --> T[Tools]
  S --> P[Prompts]
  S --> SA[Sampling]
```

<div class="text-sm text-gray-500 mt-4">
Chaque capacitÃ© est accessible via JSON-RPC.  
ğŸ’¡ Les serveurs MCP peuvent Ãªtre locaux ou distants.
</div>

---
layout: center
---

# ğŸ§± Architecture technique

| Composant            | RÃ´le                             | Exemple                                  |
| -------------------- | -------------------------------- | ---------------------------------------- |
| **MCP Client**       | LLM ou Agent IA                  | Claude 4.5, OpenAI o3, Google Gemini 2.5 |
| **MCP Server**       | Fournisseur de donnÃ©es ou outils | Google Drive, Slack, GitHub, PostgreSQL  |
| **Protocole**        | JSON-RPC 2.0                     | standard unifiÃ©                          |
| **Transport**        | HTTP / SSE / stdio               | selon contexte                           |
| **InteropÃ©rabilitÃ©** | multi-LLM / multi-vendor         | oui (open spec)                          |

<div class="mt-8 text-xl text-gray-600">
ğŸ’¡ Un serveur MCP peut Ãªtre branchÃ© Ã  plusieurs clients IA diffÃ©rents, sans adaptation spÃ©cifique.
</div>

---
layout: center
---

# ğŸ” SÃ©curitÃ© & Gouvernance

<div class="grid grid-cols-2 gap-12 mt-16 text-xl">

<div class="space-y-6">

âœ… Authentification par OAuth 2.1  
âœ… Gestion fine des permissions (â€œscopesâ€)  
âœ… SÃ©paration des contextes dâ€™exÃ©cution  
âœ… Audit et logs dâ€™appels tools/resources
</div>

<div class="space-y-6">

âš ï¸ 2025 : incidents signalÃ©s sur serveurs sans auth  
âš ï¸ Risques dâ€™â€œover-permissioningâ€ (cf. Replit, mai 2025)  
âš ï¸ Nouvelle spec : Auth obligatoire par dÃ©faut
</div>

</div>

<div class="mt-10 text-xl text-gray-600">
ğŸ’¬ Le MCP est pensÃ© â€œsecure-by-designâ€, mais nÃ©cessite une implÃ©mentation rigoureuse.
</div>

---
layout: center
---

# ğŸŒ Adoption en 2025

<div class="grid grid-cols-2 gap-12 mt-16 text-2xl">

<div class="space-y-6">

ğŸ§  **Anthropic** â†’ Claude Desktop & API  
âš¡ **OpenAI** â†’ Agents SDK (mars 2025)  
â˜ï¸ **Google** â†’ Gemini 2.5 Pro (avril 2025)  
â˜ï¸ **AWS** â†’ Steering Committee
</div>

<div class="space-y-6">

ğŸ§© 16 000+ serveurs communautaires  
ğŸ’¬ Support natif : Slack, GitHub, Notion, Google Drive  
ğŸ’¾ Connecteurs open-source : PostgreSQL, MongoDB, S3, Redis
</div>

</div>

<div class="mt-10 text-xl text-gray-600">
ğŸ’¡ En 2025, MCP devient la â€œplomberie standardâ€ du RAG et des Agents.
</div>

---
layout: center
---

# ğŸ’¡ Exemple dâ€™intÃ©gration Claude Desktop

```yaml
# ~/Library/Application Support/Claude/claude_desktop_config.json
{
    "mcpServers": {
      "weather": {
        "command": "/Users/bricefotzo/.local/bin/uv",
        "args": [
          "--directory",
          "/Users/bricefotzo/workspace/perso/weather",
          "run",
          "weather.py"
        ]
      }
    }
  }
```

<div class="text-xl text-gray-600 mt-8">
Claude 4.5 dÃ©tecte ces serveurs automatiquement  
â†’ accÃ¨s direct Ã  GitHub et PostgreSQL depuis lâ€™interface IA.
</div>

---
layout: center
---

# ğŸ’¡ Exemple dâ€™intÃ©gration Claude Desktop

```yaml
# ~/Library/Application Support/Claude/claude_desktop_config.json
{
    "mcpServers": {
      ...
      },
      "mongodb": {
        "command": "docker",
        "args": [
          "run",
          "-i",
          "--rm",
          "-e",
          "MDB_MCP_CONNECTION_STRING",
          "mcp/mongodb"
        ],
        "env": {
          "MDB_MCP_CONNECTION_STRING": "mongodb+srv://username:pwd@serial-techos.hxiqgqy.mongodb.net/"
        }
      }
    }
  }
```

<div class="text-xl text-gray-600 mt-8">
Claude 4.5 dÃ©tecte ces serveurs automatiquement  
â†’ accÃ¨s direct Ã  GitHub et PostgreSQL depuis lâ€™interface IA.
</div>

---
layout: two-cols
---

# ğŸ§© Cas dâ€™usage typiques

<div class="text-xl mt-8 space-y-6">

* ğŸ” Connecter un LLM Ã  ta base PostgreSQL / MongoDB Atlas
* ğŸ’¬ Interagir avec Slack / Jira / Notion via tools
* ğŸ“‚ Extraire et synthÃ©tiser du contenu Google Drive
* ğŸ“Š ExÃ©cuter des pipelines RAG en environnement contrÃ´lÃ©
* ğŸ§  Relier plusieurs IA Ã  des ressources partagÃ©es

</div>

::right::

```mermaid
flowchart LR
  A[Claude / GPT / Gemini] -->|MCP| B[PostgreSQL]
  A -->|MCP| C[Slack]
  A -->|MCP| D[GitHub]
  A -->|MCP| E[Google Drive]
```

<div class="text-sm text-gray-500 mt-6">
ğŸ’¡ Chaque connexion suit la mÃªme spec MCP â†’ dÃ©ploiement simplifiÃ©.
</div>


---
layout: default
---

# ğŸ”’ Bonnes pratiques de dÃ©ploiement

| Aspect               | Recommandation                           |
| -------------------- | ---------------------------------------- |
| **SÃ©curitÃ©**         | Auth obligatoire, isolation des serveurs |
| **ObservabilitÃ©**    | Logs dâ€™accÃ¨s tools/resources             |
| **Maintenance**      | Versionner les serveurs MCP              |
| **InteropÃ©rabilitÃ©** | Respect strict du JSON-RPC 2.0           |
| **Performance**      | Cache local + compression HTTP/SSE       |

<div class="mt-8 text-xl text-gray-600">
ğŸ’¡ Traiter un serveur MCP comme un microservice critique.
</div>

---
layout: default
---

# ğŸ“ En rÃ©sumÃ© â€“ Model Context Protocol (MCP)

<div class="grid grid-cols-2 gap-12 mt-12 text-xl">

<div class="space-y-6">

âœ… Standard universel pour connecter IA â†” outils / donnÃ©es  
âœ… JSON-RPC 2.0, extensible et open-source  
âœ… Compatible multi-fournisseurs (Anthropic, OpenAI, Google)  
âœ… SÃ©curisÃ© (OAuth 2.1, scopes, audit)
</div>

<div class="space-y-6">

âš ï¸ Jeune standard (2024 â†’ 2025)  
âš ï¸ Serveurs encore inÃ©gaux en qualitÃ©  
âš ï¸ NÃ©cessite une politique de sÃ©curitÃ© claire  
âš ï¸ Ã€ surveiller : MCP v2 (prÃ©vue 2026)
</div>

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
Next â†’ 3.3 : Agent-to-Agent Protocol (A2A) ğŸ¤
</div>

---
layout: section
---

# 3.3
# Agent-to-Agent Protocol (A2A)

---
layout: center
---

# ğŸ¤ Agent-to-Agent (A2A)
## Quand les IA commencent Ã  se parler

<div class="mt-12 text-2xl space-y-8">

ğŸ’¡ **A2A = protocole dâ€™Ã©change entre agents IA.**  
Permet Ã  deux (ou plusieurs) agents de **se dÃ©couvrir, communiquer et collaborer**,  
indÃ©pendamment du fournisseur (OpenAI, Google, Anthropic...).

</div>

<div class="text-xl text-gray-600 mt-8">
ConÃ§u en 2025 par <strong>Google</strong> Â· standardisÃ© par la Linux Foundation.
</div>

---
layout: center
---

# âš™ï¸ Architecture simplifiÃ©e

```mermaid
flowchart LR
  A1[ğŸ¤– Agent A] -->|HTTP + JSON-RPC| A2[ğŸ¤– Agent B]
  A2 -->|Event Stream - SSE| A1
  A1 --> U[ğŸ§‘â€ğŸ’» Utilisateur / App]
````

<div class="text-xl text-gray-600 mt-8">
Chaque agent expose une â€œAgent Cardâ€ publique (capabilities, tools, contact).  
Les autres agents peuvent la lire et interagir via A2A.
</div>

---
## layout: center
---
# ğŸ§© Concepts clÃ©s

| Ã‰lÃ©ment              | Description                                        |
| -------------------- | -------------------------------------------------- |
| **Agent Card**       | Fichier `.well-known/agent.json` dÃ©crivant lâ€™agent |
| **Discovery**        | Identification des capacitÃ©s et skills             |
| **Transport**        | HTTP + SSE + JSON-RPC                              |
| **Lifecycle**        | `submitted â†’ working â†’ completed`                  |
| **InteropÃ©rabilitÃ©** | multi-fournisseurs et open-source                  |

<div class="mt-8 text-xl text-gray-600">
ğŸ’¬ A2A = MCP entre agents, plutÃ´t quâ€™entre modÃ¨le et outils.
</div>

---
## layout: center
---
# ğŸ”— MCP vs A2A

| Aspect       | MCP                       | A2A                             |
| ------------ | ------------------------- | ------------------------------- |
| Type de lien | Agent â†” Outil / DonnÃ©es   | Agent â†” Agent                   |
| Niveau       | IntÃ©gration               | Collaboration                   |
| Exemple      | Claude â†” PostgreSQL       | Claude â†” GPT-4o                 |
| Adoption     | Anthropic, OpenAI, Google | Google, Linux Foundation (2025) |

<div class="mt-8 text-xl text-gray-600">
Les deux sont **complÃ©mentaires** et souvent utilisÃ©s ensemble.
</div>

---

# ğŸ’¡ Cas dâ€™usage concrets

<div class="grid grid-cols-2 gap-12 mt-12 text-xl">

<div class="space-y-4">

ğŸ¤ **Collaboration multi-agents**  
(ex : un planificateur + un exÃ©cuteur)  

ğŸ“Š **Coordination de tÃ¢ches inter-apps**
(ex : agent marketing â†” agent data)

</div>

<div class="space-y-4">

ğŸ”— **Interop multi-cloud**  
(OpenAI â†” Google Gemini â†” Claude)

ğŸ§  **Ã‰cosystÃ¨mes dâ€™agents distribuÃ©s**
(IBM ACP, Linux A2A, open-source)

</div>

</div>

---

# ğŸ¯ En rÃ©sumÃ© â€“ A2A

<div class="grid grid-cols-2 gap-12 mt-12 text-xl">

<div class="space-y-4">

âœ… Communication directe entre agents IA  
âœ… Standard ouvert et extensible  
âœ… BasÃ© sur JSON-RPC & HTTP  
âœ… ComplÃ©ment naturel du MCP
</div>

<div class="space-y-4">

âš ï¸ Jeune standard (2025)  
âš ï¸ Authentification et sandbox encore limitÃ©es  
âš ï¸ Gouvernance multi-LLM en construction
</div>

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
Next â†’ Bloc 4 : Production & SÃ©curitÃ© ğŸ”’
</div>

---
layout: section
---

# 4.1
# Monitoring & ObservabilitÃ©

---
layout: center
---

# ğŸ“Š Pourquoi monitorer un systÃ¨me IA ?

<div class="mt-12 text-2xl space-y-8">

ğŸ’¡ En production, **un LLM sans monitoring = une boÃ®te noire**.  
Il faut suivre :
- ses **performances**
- ses **coÃ»ts**
- et la **qualitÃ© des rÃ©ponses**

â¡ï¸ Lâ€™observabilitÃ© = pilier de la fiabilitÃ© et de la scalabilitÃ©.

</div>

<div class="text-xl text-gray-600 mt-8">
ğŸ¯ Objectif : dÃ©tecter erreurs, dÃ©rives, coÃ»ts excessifs et dÃ©gradations qualitÃ©.
</div>

---
layout: two-cols
---

# ğŸ§  Les 3 dimensions du monitoring

<div class="text-xl mt-8 space-y-8">

### 1ï¸âƒ£ Performance  
- Latence (temps moyen, TTFT)  
- DÃ©bit (req/sec, tokens/sec)  
- DisponibilitÃ© (% uptime)

### 2ï¸âƒ£ CoÃ»t  
- Tokens input/output  
- Cache hit-rate  
- CoÃ»t par requÃªte / utilisateur

### 3ï¸âƒ£ QualitÃ©  
- Hallucination rate  
- Faithfulness (vÃ©racitÃ©)  
- Pertinence (score humain / RAGAS)

</div>

::right::

```mermaid
graph TD
  A[Performance] --> D[ObservabilitÃ©]
  B[CoÃ»t] --> D
  C[QualitÃ©] --> D
  D --> R[ğŸš¦ FiabilitÃ© globale]
```

<div class="text-sm text-gray-500 mt-4">
Les 3 axes sont liÃ©s : rÃ©duire le coÃ»t sans casser la qualitÃ©.
</div>

---
layout: two-cols
---

# ğŸ“ˆ Outils et plateformes

<div class="text-xl mt-8 space-y-6">

### ğŸ”¹ LangSmith (LangChain)

* Traces, latence, tokens, erreurs
* Comparaison A/B de prompts
* Replay & versioning des runs

### ğŸ”¹ Phoenix (Arize AI)

* Analyse visuelle des embeddings
* DÃ©tection de dÃ©rives / anomalies

### ğŸ”¹ TruLens

* Ã‰value la qualitÃ© (faithfulness, relevance)
* Supporte RAG, CoT, multi-models

</div>

::right::


<div class="text-xl mt-8 space-y-6">

### ğŸ”¹ Weights & Biases

* Tracking ML + LLM
* Dashboards + alertes

### ğŸ”¹ OpenTelemetry / Prometheus

* Instrumentation standard (latence, erreurs)
* IntÃ©gration avec Grafana

### ğŸ”¹ Datadog / New Relic

* Logs + alertes production
* CorrÃ©lation application / API / infra

</div>

<div class="mt-8 text-sm text-gray-500">
ğŸ’¡ En pratique : un mix LangSmith + OpenTelemetry = couverture complÃ¨te.
</div>

---
## layout: two-cols
---
# ğŸ§© Exemple : pipeline instrumentÃ© (RAG)

<div class="text-xl mt-8 space-y-6">

Instrumentation avec **OpenTelemetry** :

```python
from opentelemetry import trace

tracer = trace.get_tracer("rag-pipeline")

with tracer.start_as_current_span("query"):
    context = retriever.get_docs(query)
    response = llm.invoke(context)
```

â†’ permet de suivre :

* durÃ©e du retrieval
* temps dâ€™infÃ©rence
* erreurs ou timeouts

</div>

::right::

```mermaid
flowchart LR
  Q[User Query] --> R[Retrieval Span]
  R --> L[LLM Span]
  L --> O[Response Span]
  subgraph Metrics
    A1[Latency]:::m
    A2[Tokens]:::m
    A3[Errors]:::m
  end
  classDef m fill:#def,stroke:#333;
```

<div class="text-sm text-gray-500 mt-6">
ğŸ’¡ Chaque Ã©tape du pipeline devient observable via traces et spans.
</div>

---
## layout: center
---
# ğŸ§® Table de mÃ©triques clÃ©s

| CatÃ©gorie   | Indicateur                 | Objectif                    |
| ----------- | -------------------------- | --------------------------- |
| Performance | Latence moyenne            | < 1 s pour requÃªtes courtes |
| Performance | TTFT (time to first token) | < 300 ms (vLLM / TGI)       |
| CoÃ»t        | Tokens moyens / requÃªte    | suivre dÃ©rive               |
| CoÃ»t        | Cache hit rate             | > 70 %                      |
| QualitÃ©     | Faithfulness score         | > 0.85                      |
| QualitÃ©     | Hallucination rate         | < 5 %                       |

<div class="mt-10 text-xl text-gray-600">
ğŸ’¬ Ces valeurs guides servent Ã  dÃ©tecter les dÃ©rives avant lâ€™utilisateur.
</div>

---
## layout: center
---
# ğŸ§  Ã‰valuation qualitative automatisÃ©e

<div class="mt-12 text-2xl space-y-8">

Les outils comme **TruLens** ou **LangSmith Evaluate**
utilisent un LLM pour **noter les rÃ©ponses dâ€™un autre modÃ¨le**.

> â€œCette rÃ©ponse est-elle correcte, claire et cohÃ©rente avec le contexte ?â€

ğŸ’¡ Approche â€œLLM-as-a-Judgeâ€ en production,
combinÃ©e Ã  des feedbacks humains (calibration).

</div>

---

# âš™ï¸ A/B Testing & ExpÃ©rimentation

<div class="grid grid-cols-2 gap-12 mt-16 text-xl">

<div class="space-y-6">

âœ… Comparer deux versions de prompt / modÃ¨le  
âœ… Mesurer impact sur qualitÃ© / coÃ»t / latence  
âœ… Stocker les runs pour rÃ©gression tests  
</div>

<div class="space-y-6">

âš ï¸ Toujours isoler les tests (dataset stable)  
âš ï¸ Utiliser sampling cohÃ©rent  
âš ï¸ Ne jamais dÃ©ployer un nouveau prompt sans Ã©valuation  
</div>

</div>

<div class="mt-10 text-xl text-gray-600">
ğŸ’¡ Le prompt est un composant logiciel : il se teste et se versionne.
</div>

---
## layout: center
---
# ğŸ§© Pipeline complet observÃ©

```mermaid
flowchart LR
  A[User Request] --> B[FastAPI Endpoint]
  B --> C[LangChain Chain]
  C --> D[LiteLLM Gateway]
  D --> E[(LLM)]
  C --> F[(Vector Store)]
  B --> G[Monitoring Stack<br/>Prometheus + Grafana + LangSmith]
```

<div class="mt-10 text-xl text-gray-600">
ğŸ’¡ ObservabilitÃ© intÃ©grÃ©e = comprÃ©hension + confiance + scalabilitÃ©.
</div>

---

# ğŸ¯ En rÃ©sumÃ© â€“ Monitoring & ObservabilitÃ©

<div class="grid grid-cols-2 gap-12 mt-12 text-xl">

<div class="space-y-4">

âœ… Mesurer performance, coÃ»t et qualitÃ©  
âœ… Utiliser outils : LangSmith, TruLens, Phoenix  
âœ… Instrumenter avec OpenTelemetry  
âœ… A/B tester et Ã©valuer rÃ©guliÃ¨rement  
</div>

<div class="space-y-4">

âš ï¸ Sans observabilitÃ© : dÃ©rives silencieuses  
âš ï¸ QualitÃ© â‰  prÃ©cision brute  
âš ï¸ Surveiller le coÃ»t par requÃªte et cache hit-rate  
</div>

</div>

<div class="absolute bottom-10 left-1/2 transform -translate-x-1/2 text-xl text-gray-500">
Next â†’ 4.2 : SÃ©curitÃ© ğŸ”’
</div>
